{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JxOCySO-KtL",
        "outputId": "121c01b6-3fe9-435c-912f-f8a1ceaca531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Write a short motivational quote about Success: \"If YOU want a lifetime of success, start now!\"'}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "quote_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "theme = \"Success\"\n",
        "prompt = f\"Write a short motivational quote about {theme}:\"\n",
        "result = quote_generator(\n",
        "    prompt,\n",
        "    max_length=40,\n",
        "    do_sample=True,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ka0Avu6D-WZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}